# -*- coding: utf-8 -*-
"""Neural_Network_Intro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ue52UNJuZ8LufnSdZ_tOLU0qIFs7eq7

# **CSE422 Lab: Neural Networks**

## **Neural Networks**
In machine learning, a neural network (also artificial neural network abbreviated ANN) is a model inspired by the structure and function of biological neural networks in animal brains.

### **Forward Propagation**

The forward propagation of a single layer can be mathematically expressed as follows:

Let $f$ be the activation function. Then, for the $l^{th}$ layer, the input is $A^{[l-1]}$ and the output $A^{[l]}$ is computed as:

$$Z^{[l]} = W^{[l]}A^{[l-1]} + B^{[l]}$$
$$A^{[l]} = f(Z^{[l]})$$

where $W^{[l]}$ is the weight matrix connecting $(l-1)^{th}$ layer to $l^{th}$ layer, $B^{[l]}$ is the bias matrix of $l^{th}$ layer, and $Z^{[l]}$ is the pre-activation output $l^{th}$ layer.

### **Backpropagation**

The backpropagation of a single layer can be mathematically expressed as follows:

Let $Y$ be the output, $f$ be the activation function, and $E$ be the cost function. The upstream gradient for the output layer $D_u^{[L]}$, where $L$ is the number of layers in the network, is computed as:

$$D_u^{[L]} = \nabla_Y E$$

Let $D^{[l]} = \nabla_{Z^{[l]}} E$, where, $Z^{[l]}$ be the pre-activation output of $l^{th}$ layer. For $l^{th}$ layer, $D^{[l]}$ is computed as:

$$D^{[l]} = D_u^{[l]} \odot f'(Z^{[l]})$$

Then, the partial derivatives of the cost function with respect to weight matrix $W^{[l]}$ and bias matrix $B^{[l]}$ are computed as:

$$\nabla_{W^{[l]}} E = D^{[l]} {A^{[l-1]}}^\mathsf{T}$$
$$\nabla_{B^{[l]}} E = D^{[l]}$$

where $A^{[l-1]}$ is the input from $(l-1)^{th}$ layer. Finally, the weight matrix $W^{[l]}$ and bias matrix $B^{[l]}$ are updated using the following equations:

$$W^{[l]} \rightarrow W^{[l]} - \eta \nabla_{W^{[l]}} E$$
$$B^{[l]} \rightarrow B^{[l]} - \eta \nabla_{B^{[l]}} E$$

where $\eta$ is the learning rate of the network.

In the end, for $(l-1)^{th}$ layer, the upstream gradient $D_u^{[l-1]}$ is computed as:

$$D_u^{[l-1]} = {D^{[l]} W_1^{[l]}}^\mathsf{T}$$

## **Implementation**

### **Necassary Libraries**
"""

import numpy as np

from tensorflow.keras.datasets import boston_housing
from tensorflow.keras.datasets import mnist
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy

import pandas as pd
from matplotlib import pyplot as plt

"""### **Activation Function**

Sigmoid --> Binary lassification(Hidden layer e use hoi na)

tanh--> Classification(Hidden layer e use hoi na)

Softmax--> Multiclass classification-->output layer

ReLU--> Hidden layer
"""

class ReLU:
    # Apply activation function
    def forwardPropagation(self, inp: np.array) -> np.array:
        self.inputAct = np.maximum(0, inp)
        return self.inputAct

    # Calculate derivative of the activation function
    def backPropagation(self, delta: np.array) -> np.array:
        return delta * np.where(self.inputAct > 0, 1, 0)

"""### **Loss Function**

Binary Cross Entropy --> Binary Classification

Categorical Cross Entropy --> Multiclass Classification

Mean Square Error --> Regression
"""

class MeanSquareError:
  def loss(self, Y: np.array, y:np.array) -> np.array:
      self.Y = Y
      self.y = y
      return np.absolute((Y - y) ** 2).sum() / y.shape[0]

  # Calculate derivative of the loss function
  def deriv(self) -> np.array:
      return (self.Y - self.y) / self.y.shape[0]

"""### **Optimizer**

Adam

GradientDescent
"""

class GradientDescent:
    def __init__(self, learningRate) -> None:
        self.learningRate = learningRate

    # Calculate gradients
    def gradients(self, gradients) -> np.array:
        return self.learningRate * gradients

"""### **Necassary Functions**"""

# To get activation function
def getActivationFunction(activation: str) -> object:
    if activation == "RELU":
        return ReLU
    else:
        raise Exception("Cannot find the activation function.")

# To get loss function
def getLossFunction(loss: str) -> object:
    if loss == "MSE":
        return MeanSquareError
    else:
        raise Exception("Cannot find the loss function.")

# To get optimizer
def getOptimizer(optimizer: str) -> object:
    if optimizer == "GD":
        return GradientDescent
    else:
        raise Exception("Cannot find the optimizer.")

"""### **Single Layer of Neurons**"""

class NeuronLayer:
    def __init__(self, inputNeurons, outputNeurons, activation, biasFlag = True, randomState = 42) -> None:
        np.random.seed(randomState)
        # set variables
        self.inputNeurons = inputNeurons
        self.outputNeurons = outputNeurons
        self.biasFlag = biasFlag

        # get activation
        self.activation = getActivationFunction(activation)()

        # initiate parameters
        self.weights = self.__initParameters((self.inputNeurons, self.outputNeurons))
        self.bias = self.__initParameters((1, self.outputNeurons))

    # Parameter initializer
    def __initParameters(self, dimension: tuple) -> np.array:
        return np.random.uniform(-np.sqrt(2/(dimension[0] + dimension[1])), np.sqrt(2/(dimension[0] + dimension[1])),size = (dimension[0], dimension[1]))

    def build(self, optimizer, learningRate) -> None:
        self.learningRate = learningRate
        self.optimizer = getOptimizer(optimizer)(learningRate)

    # Apply forward propagation
    def forwardPropagation(self, X: np.array) -> np.array:
        self.X = X

        # Calculate weighted sum
        self.output = np.dot(self.X, self.weights) + (self.biasFlag * self.bias)
        # Apply activation function
        self.output = self.activation.forwardPropagation(self.output)

        return self.output

    # Apply backpropagation
    def backPropagation(self, upstreamGradient: np.array) -> np.array:
        # Calculate activation gradients
        delta = self.activation.backPropagation(upstreamGradient)

        # Calculate parameter gradients
        weightGrad = np.dot(self.X.T, delta) / self.X.shape[0]
        biasGrad = np.dot(np.ones((1, self.X.shape[0])), delta) /  self.X.shape[0]

        # Update parameters
        self.weights -= self.optimizer.gradients(weightGrad)
        self.bias -= self.optimizer.gradients(biasGrad)

        # Calculate downstream gradients
        downstreamGradient = np.dot(delta, self.weights.T) / self.X.shape[0]

        return downstreamGradient

"""### **Model**"""

class Model:
    # Create mini-batches
    def __createBatch(self, X: np.array, Y: np.array, batchSize: int) -> tuple:
        miniX, miniY = np.array([X[:batchSize]]), np.array([Y[:batchSize]])

        for idx in range(1, X.shape[0] // batchSize):
            miniX = np.append(miniX, np.array([X[idx * batchSize : (idx + 1) * batchSize]]), axis = 0)
            miniY = np.append(miniY, np.array([Y[idx * batchSize : (idx + 1) * batchSize]]), axis = 0)

        return miniX, miniY

    # Apply forward propagation over the model
    def __forwardPropagation(self, X: np.array) -> np.array:
        output = X
        for layer in self.layers:
            output = layer.forwardPropagation(output)
        return output

    # Apply backpropagation over the model
    def __backPropagation(self, Y: np.array) -> None:
        gradient = Y
        for layer in self.layers[::-1]:
            gradient = layer.backPropagation(gradient)

    def layers(self, layers: list) -> None:
        self.layers = layers

    # Compile the model
    def compile(self, loss, optimizer, learningRate) -> None:
        self.loss = getLossFunction(loss)()
        for layer in self.layers[::-1]:
            layer.build(optimizer, learningRate)

    # Predict the output
    def predict(self, X: np.array) -> np.array:
        return self.__forwardPropagation(X)

    # Evaluate the model
    def evaluate(self, X: np.array, Y: np.array) -> np.array:
        output = self.predict(X)
        return self.loss.loss(output, Y)

    # Train the model
    def fit(self, X: np.array, Y: np.array, epochs: int, batchSize = None) -> np.array:
        # Create mini-batch
        batchSize = (batchSize if batchSize else X.shape[0])
        self.X, self.Y = self.__createBatch(X, Y, batchSize)

        self.error = np.array([])
        # Run epoch
        for epoch in range(epochs):
            epochError = np.array([])

            # Iterate over mini-batches
            for idx in range(self.X.shape[0]):
                epochError = np.append(epochError, self.loss.loss(self.__forwardPropagation(self.X[idx]), self.Y[idx]))
                self.__backPropagation(self.loss.deriv())

            # Print epoch information
            epochError /= epochError.shape[0]
            self.error = np.append(self.error, epochError.sum() / epochError.shape[0])
            print("Epoch:", epoch + 1, "Error:", round(epochError[0], 2))

        return self.error

"""### **Load Dataset**"""

# Import dataset
(X_train, Y_train), (X_test, Y_test) = boston_housing.load_data(path='boston_housing.npz', test_split=0.2, seed=42)
# Reshape target variable
Y_train, Y_test = Y_train.reshape(Y_train.shape[0], 1), Y_test.reshape(Y_test.shape[0], 1)

# Create Dataframe
dataset = pd.DataFrame(np.concatenate((X_train, X_test), axis=0))
dataset['Target'] = np.concatenate((Y_train, Y_test), axis = 0)
dataset

"""### **Model Training**"""

# Create model
model = Model()
model.layers = [NeuronLayer(13, 64, "RELU"), NeuronLayer(64, 1, "RELU")]
# Compile model
model.compile(loss="MSE",
              optimizer="GD",
              learningRate=0.01)

# Train model
training_Error = model.fit(X_train, Y_train, epochs=100, batchSize=None)

"""### **Model Evaluation**"""

# Plot loss curve
plt.plot(training_Error)
plt.title("Loss Curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()

#Print test error
print(f"Testing Error: {round(model.evaluate(X_test, Y_test), 2)}")

"""## **Tensorflow for Neural Networks**

TensorFlow is an open-source machine learning framework developed by Google. It provides tools and libraries to build and train neural networks and other machine learning models, especially for deep learning tasks.

### **Load Dataset**
"""

(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

fig, axes = plt.subplots(1, 5, figsize=(12, 12))

# Insert images
for i in range(5):
    axes[i].imshow(X_train[i])

plt.show()

"""### **Preprocess Data**"""

X_train, X_test, Y_train, Y_test = X_train.astype("float32") / 255, X_test.astype("float32") / 255, Y_train.astype("float32"), Y_test.astype("float32")

"""### **Model Training**"""

# Create model
#dense -> 128 indicates --> we send 128 data in a batch through forward propagation and use backward to recheck
#similar way continue this process untill our dataset is empty
model = Sequential([
  Flatten(input_shape=(28, 28)),
  Dense(128, activation='relu'),
  Dense(10, activation="softmax")
])

#dense, Adam, epochs er val model er upor base kore change hobe

# Compile model
model.compile(optimizer=Adam(0.001),
              loss=SparseCategoricalCrossentropy(from_logits=True),#used Categorical Cross Entropy
              metrics=['accuracy'])

# Train model
history = model.fit(X_train, Y_train, epochs = 10, batch_size=128)

"""### **Model Evaluation**"""

# Plot loss curve
plt.plot(history.history['loss'])
plt.title("Loss Curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()

#Print test error
print(f"Testing Accuracy: {round(model.evaluate(X_test, Y_test)[1] * 100, 2)}%")