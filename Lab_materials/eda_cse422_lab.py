# -*- coding: utf-8 -*-
"""EDA CSE422 Lab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w8E4De7WNsZJ4qzavf2wMV4SnI5jGqSh

# **What is EDA?**

EDA stands for **Exploratory Data Analysis**. It's the process of examining and understanding a dataset before you start modeling or drawing conclusions. EDA involves summarizing the main characteristics of the data using visual methods (like histograms, box plots, scatter plots) and statistical techniques (like mean, median, standard deviation, correlation, etc.).

**Why EDA is Important:**

*Understanding the Data:*

Before building any model, you need to know what you're dealing with. EDA helps you understand the structure, patterns, and relationships in your data.

*Identifying Data Quality Issues:*

It helps detect:

1. Missing values

2. Outliers

3. Duplicates

4. Inconsistent data types

*Guiding Feature Engineering:*

EDA can show which features might be useful, which ones are redundant, and how variables interact with each other. This insight is crucial for creating meaningful features.

*Detecting Trends and Patterns:*

EDA reveals trends, seasonality, or clusters that can influence modeling decisions.

*Avoiding Wrong Assumptions:*

Looking at the raw data can prevent mistakes like assuming normal distribution when the data is skewed.

*Improving Model Performance:*

Better understanding of your data often leads to better preprocessing, which leads to more accurate models.

**Basic EDA Pipeline**

*   Explore the dataset.
*   Understand its structure and contents.
*   Discover patterns, relationships and anomalies.
*   Prepare it for further modeling or decision-making.
"""

from google.colab import drive
drive.mount('/content/drive')

"""### **Stages of EDA**
####**Common EDA Techniques:**
#### 1.Descriptive Analysis
#### 2.Correlation Analysis
#### 3.Check imbalance in data

#### **Importing Required Python libraries**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""### **Load dataset**"""

dataset=pd.read_csv("/content/drive/MyDrive/EDA Lab CSE422/modified_train.csv")

"""### **Summarize data**"""

dataset

dataset.head(10)

print ('Shape of the dataset is {}. This dataset contains {} rows and {} columns.'.format(dataset.shape,dataset.shape[0],dataset.shape[1]))

"""### **Feature Names and its Datatypes**"""

dataset.info()

"""### **Data Spliting**
#### Select and separately store Numerical and Categorical features in different variables.
"""

##Selecting numerical features
numerical_data = dataset.select_dtypes(include='number')

#append the features of numerical_data to list
numerical_features=numerical_data.columns.tolist()

print(f'There are {len(numerical_features)} numerical features:', '\n')
print(numerical_features)

#Selecting categoricalfeatures
categorical_data=dataset.select_dtypes(include= 'object')

#append the features of categorical_data to list
categorical_features=categorical_data.columns.tolist()

print(f'There are {len(categorical_features)} numerical features:', '\n')
print(categorical_features)

"""### **Descriptive Analysis**
#### In descriptive Analysis we analysis each variable separately to get inference about the feature.
### **Summary satistics of Numerical Features**
"""

# Transposed stats for numerical features

numerical_data.describe().T

"""### **Summary satistics of Categorical features**"""

# Transposed stats for categorical features

categorical_data.describe().T

"""### **Variance of each numerical features**"""

numerical_data.var()

"""### **Skew in numerical features**

üìê What is Skewness?
Skewness measures the asymmetry of the distribution of a dataset.

Skewness = 0 ‚Üí Perfectly symmetrical (e.g., normal distribution)

Skewness < 0 ‚Üí Left-skewed (long tail on the left)

Skewness > 0 ‚Üí Right-skewed (long tail on the right)

It helps you understand how balanced your data is and whether transformations are needed.

üìä Interpretation of Skewness Values:

Skewness Value	Interpretation

~ 0	Symmetrical (normal-ish)

0 to ¬±0.5	Fairly symmetrical

¬±0.5 to ¬±1	Moderately skewed

greater than ¬±1	Highly skewed
"""

numerical_data.skew()

"""###**Skewness Interpretation**

*ID (0.448): Fairly symmetrical ‚Äì no strong skew.

*CREDIT_SCORE (-0.226): Slightly left-skewed ‚Äì not a concern.

*VEHICLE_OWNERSHIP (-1.729): Highly left-skewed ‚Äì most values are 1 or 0; likely a categorical variable.

*MARRIED (-0.341): Slightly left-skewed ‚Äì not a big issue.

*CHILDREN (-0.080): Almost symmetrical ‚Äì no transformation needed.

*POSTAL_CODE (2.987): Strong right-skew ‚Äì likely due to many unique codes; should be treated as categorical.

*ANNUAL_MILEAGE (-0.510): Moderately left-skewed ‚Äì may benefit from transformation.

*SPEEDING_VIOLATIONS (3.900): Highly right-skewed ‚Äì most people have few or no violations; some have many.

*DUIS (5.609): Extremely right-skewed ‚Äì majority have zero DUIs; a few have multiple.

*PAST_ACCIDENTS (4.346): Strong right-skew ‚Äì most have few accidents; some have many.

*OUTCOME (0.313): Fairly symmetrical ‚Äì distribution looks balanced.

### **Histograms and Box Plot**
#### To find the distributions and outlier in the each feature
"""

numerical_data.hist(figsize=(12,12),bins=20)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Select only numerical columns for boxplot analysis
numeric_cols = dataset.select_dtypes(include=['int64', 'float64']).columns

# Set up the figure
plt.figure(figsize=(20, 30))

# Plot boxplots for each numerical feature including the target variable 'OUTCOME'
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(len(numeric_cols), 1, i)
    sns.boxplot(x=dataset[col], color='skyblue')
    plt.title(f'Boxplot of {col}', fontsize=12)
    plt.tight_layout()

plt.show()

"""### **Number Unique values in each feature**"""

numerical_data.nunique()

"""### **Missing Values**"""

numerical_data.isnull().sum()

"""### **Categorical Features**

#### **No of unique values in each categorical feature**
"""

# unique values counts
unique_counts=categorical_data.nunique()
print(unique_counts)

"""### **Barplot of unique value counts in every categorical features**"""

for col in categorical_features:
    plt.title(f'Distribution of {col}')
    categorical_data[col].value_counts().sort_index().plot(kind='bar', rot=0, xlabel=col,ylabel='count')
    plt.show()

"""### **Correlation Analysis**
### **Correlation matrix of whole dataset**
"""

# Calculate the correlation matrix
correlation_matrix = numerical_data.corr()
correlation_matrix

"""### **Correlation Heatmap plot of whole dataset**"""

# Plotting the heatmap for correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.3f', linewidths=0.3)
plt.show()

"""### **Generating correlation plot between features and target variable using different method**

### **Some Correlation techniques**
**üîπ 1. Pearson Correlation Coefficient**

üìå Purpose:
Measures the linear relationship between two continuous (numeric) variables.

üí° Characteristics:

Value ranges from -1 to +1

+1: Perfect positive linear correlation

-1: Perfect negative linear correlation

0: No linear relationship

Sensitive to outliers.

‚úÖ Used For:
Finding how variables like CREDIT_SCORE, ANNUAL_MILEAGE, DUIS, etc., relate to each other or the OUTCOME.

**üîπ 2. Cram√©r's V (for Categorical-Categorical Correlation)**

üìå Purpose:
Measures the association between two categorical variables.

üí° Characteristics:

Value ranges from 0 to 1

0: No association

1: Perfect association

Based on Chi-Square statistics

‚úÖ Used For:
Checking relationships between columns like GENDER, EDUCATION, INCOME, TYPE_OF_VEHICLE, etc.

**üîπ 3. Point-Biserial Correlation (for Continuous vs Binary)**

üìå Purpose:
A special case of Pearson correlation used when one variable is binary (e.g., 0 or 1) and the other is continuous.

üí° Characteristics:

Essentially checks if the means of the two binary groups are different in a statistically linear sense.

‚úÖ Used For:
Relating binary target OUTCOME with features like:

* CREDIT_SCORE

* ANNUAL_MILEAGE

* PAST_ACCIDENTS

* etc.

**üîπ 4. Spearman Rank Correlation**

üìå Purpose:
Measures monotonic relationships between two variables ‚Äî whether linear or non-linear.

üí° Characteristics:

Uses rank values rather than raw data.

Value range: -1 to +1

Less sensitive to outliers than Pearson

Ideal for ordinal data or when relationships are not linear.

‚úÖ Use Case:
DRIVING_EXPERIENCE (ordinal) vs OUTCOME

AGE group vs CREDIT_SCORE

**üîπ 5. Kendall‚Äôs Tau**

üìå Purpose:
Also measures ordinal association between two variables using pairwise concordance.

üí° Characteristics:

More conservative than Spearman

Interprets strength of monotonic relationships

Value range: -1 to +1

Better for smaller datasets or when many ties exist in the ranks

‚úÖ Use Case:
Similar to Spearman ‚Äî works well with ranks and ties.

#### **Correlation plot between numerical features and target**
"""

fig, ax = plt.subplots(3,1, figsize=(10, 10))
## Correlation coefficient using different methods
corr1 = numerical_data.corr('pearson')[['OUTCOME']].sort_values(by='OUTCOME', ascending=False)
corr2 = numerical_data.corr('spearman')[['OUTCOME']].sort_values(by='OUTCOME', ascending=False)
corr3 = numerical_data.corr('kendall')[['OUTCOME']].sort_values(by='OUTCOME', ascending=False)

#setting titles for each plot
ax[0].set_title('Pearson method')
ax[1].set_title('spearman method')
ax[2].set_title('Kendall method')

## Generating heatmaps of each methods
sns.heatmap(corr1, ax=ax[0], annot=True)
sns.heatmap(corr2, ax=ax[1], annot=True)
sns.heatmap(corr3, ax=ax[2], annot=True)

plt.show()

"""### **Check imbalance in the data**
#### We have classification problem so we need to check the balance of the given data.
"""

#check Imbalance in data

#group instances based on the classes in OUTCOME variable
class_counts=dataset.groupby("OUTCOME").size()

columns=['outcome','count','percentage']
outcome=[0,1]
count=list()
percentage=list()

#Calculate the percentage of each value of the OUTCOME variable from total
for val in range(2):
    count.append(class_counts[val])
    percent=(class_counts[val]/105000)*100
    percentage.append(percent)

# Convert the calulated values into a dataframe
imbalance_df=pd.DataFrame(list(zip(outcome,count,percentage)),columns=columns)
imbalance_df

"""### **Barplot of Outcome vs Percentage**"""

sns.barplot(data=imbalance_df,x=imbalance_df['outcome'],y=imbalance_df['percentage'])
plt.show()

"""### **Observation**
#### * Based on the above grouping, It shows that around **58%** of instances consists OUTCOME value '0' and **42%** of instances consists OUTCOME values '1'
#### * There is imbalance between two classes **(approx 16% difference)**

###**Common Solutions to Class Imbalance:**
‚úÖ 1. Resampling Techniques


a. Oversampling (Minority Class)

Increases the number of samples in the minority class.

Tools: SMOTE, ADASYN, or simple random oversampling.

b. Undersampling (Majority Class)

Reduces the number of samples in the majority class.

Can risk losing important information.

c. Hybrid Methods

Combines both oversampling and undersampling for better balance.

Example: SMOTE-ENN, SMOTE-Tomek Links.

‚úÖ 2. Use of Class Weights

Assign higher penalty to misclassifying minority class. Many ML algorithms support this:

class_weight='balanced' in LogisticRegression, RandomForest, SVM, etc.

In deep learning: use weighted loss functions (e.g., weighted Cross Entropy).

‚úÖ 3. Synthetic Data Generation

Create artificial data points for minority class.

SMOTE (Synthetic Minority Over-sampling Technique) is the most popular method.

ADASYN is an advanced version of SMOTE that adapts based on the data distribution.

‚úÖ 4. Anomaly Detection Framing

If the minority class is extremely rare, consider treating it as an anomaly detection problem.

‚úÖ 5. Ensemble Methods

Bagging/Boosting techniques (e.g., Balanced Random Forest, XGBoost with scale_pos_weight) can perform well on imbalanced data.

These methods can learn patterns even in imbalanced settings by focusing on hard-to-classify instances.

‚úÖ 6. Data Augmentation (for Images, Text, etc.)

Apply transformations to minority class data to artificially expand it (for example, rotate/flip images).

‚úÖ 7. Evaluation Metrics Adjustment

Accuracy can be misleading in imbalanced data.

Use:

Precision, Recall, F1-Score

ROC-AUC

PR AUC (for very imbalanced datasets)

### **Data visualization**
#### Some other visualizations of features to get more insights

### **Density plots of numerical features**
"""

numerical_data.plot(kind='density',figsize=(14,14),subplots=True,layout=(6,2),title="Density plot of Numerical features",sharex=False)
plt.show()

"""### **SUMMARY**
#### Performed various exploratory data analysis techniques such as univariate, correlation, visualization on the given vehicle insurance dataset also found moderate imbalance in the given dataset. From the insights acquired through the analysis we will make better decisions when we do Machine learning model development.
"""